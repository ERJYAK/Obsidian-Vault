Это [распределенная](Общее#^e6826a) NoSQL база данных.
Она устроена по **master-less** архитектуре для того, чтобю добиться высокой отказоустойчивости. Каждая нода умеет исполнять обязанности ее соседки. Принципиально не имеет единой точки отказа.

Все узлы взаимодействуют друг с другом. Вместе все ноды одного датацентра образуют кольцо. Они учитывают состояния друг друга - если соседний узел выходит из строя, то нода будет накапливать для соседа данные (hinted handoff), чтобы доставить их соседу при его восстановлении.

**Свойства:**
1. Реплицируемая. Приводит к высокой отказоустойчивости.
2. Tunable consistency (Настраиваемая консистентность). 

В **Cassandra** в основном используется горизонтальное масштабирование. Она масштабируется линейной. Очень легко добавлять новые ноды увеличивая мощность БД.

На основе поля, выбранного как `partition_key`, данные будут размещены на разных серверах и объеденины в **партиции.** Получается так, что мы не ограничены в количестве данных. Если добавляется новая нода - кластер сам управляет распределением **партиций**.

Помимио того, что данные распределены, они еще и реплицированы. В целях увеличения отказоустойчивости и увеличения кол-ва серверов, которые могут отвечать на определенные запросы, одна и та же запись/партиция будут находится на нескольких нодах в зависимотси от настроек [`replication factor`](#^100000).

**При загрузке драйвер считывает распределение данных и схему размещения кластера.**

Можно создать таблицу с `sequence_id`, но при этом всем нодам необходимо будет договариваться об инкрименте, что замедляет процесс записи. Поэтому в кассандре ключи создаются самостоятельно. Например есть тип `uuid` - universally unique id, огромное число и символы, которое считается унифверсальным и уникальным.
Первичный ключ в **Cassandra** состоит из 2х частей - [`partition key`](#^100001) и `clustering column`. `clustering column` может и не присутствовать в ключе.

## Типичные случаи применения

**Масштабируемость** и все что связано с масштабируемостью. Высокая производительность и высокий объем данных $\rightarrow$ много `write` и `read` операций $\rightarrow$ логи и временные ряды. 

**Доступность** $\rightarrow$ данные должны быть доступны всегда и никогда не должны быть потеряны

**Распределенность** $\rightarrow$ все случаи применения связанные с отслживанием и логистикой

Кассандра про высокую нагрузку, высокую надежность и географическую распределенность. Она не подходит для аналитических целей. Аналитика и поиск не будут легкой задачи.

# Модель данных

Это табличная модель.

`Keyspace` - сама базы данных, например как схема в **Postgres**. Она содержит табдлицы.

Внутри таблицы находятся `partitions` - разделы. Такой раздел содержит группу строк, которая хранится на одной ноде.

Каждая строка содержит `partition key`. Это одна или несколько колонок (полей), которые хэшируются для определния, в какой ноде хранить партицию. 
^100001

`clustering column` необходим для следующих вещей:
1. Удостовериться в уникальности ключа.
	**PK** должен быть уникальным. Иначе старые данные могут быть затерты через [upsert](Термины и сущности для разбора#) 
2. Определяет порядок сортировки.

Создание таблицы выглядит следующим образом:
```cql
create table population_table(
	country text,
	city text,
	population int,
	PRIMARY KEY((country), city)
)
```
В данном случае `country` - `partition key`; `city` - `clustering column`.

Важно правильно распределять и определять разделы. Так как может получитсья ситуация, когда раздел перегружен или постоянно обновляется. Разделы (партиции) должны быть устроены таким образом, чтобы распределять данные равномерно.

#### Репликация внутри кольца

Есть кольцо из нескольких узлов. Данные ложатся в каждый узел в соответствии со своими партициями. Если приходят данные, приходят на любую ноду. Каждая нода способна выполнить операцию

Сначала определяется на какой сервер отправлять данные. Необходимо узнать на какую ноду отправлять данные и нак какие ноды их реплицировать.
Координатор нода (получившая данные) берет `partition key` и запроса, высчитывает его хэш и согласно имеющимя данным схемы понимает, какие ноды ответствены за этот диапазон. Такая информоция (про ответственность) получается до поступления запроса через `gossip` проактивно.
Репликация происходит [асинхронно](Общее#^a00001) на репликационные ноды.

**Replica node** - нода, ответственная за данные и их репликацию.
# Настройки

**replication factor** определяет на какое число нод внутри кластера будут реплицироваться (копироваться) данные. Рекомендуется использовать 3 или 5. ^100000
***
**graceful period** — это период времени, который используется для обеспечения плавной и безопасной остановки или перезапуска узла кластера, чтобы минимизировать риски потери данных или нарушения работы системы.
^100002

В период `graceful shutdown` **Cassandra** распределяет нагрузку между оставшимися узлами кластера. Для настройки длительности этого времени применяется **graceful period**.

# Защита от неконсистентности данных

При репликации данных на ноды может случится так, что одна или несколько нод недоступны. В этом случае может нарушится консистенстнтность данных, так как можем обновить только часть нод.

**Cassandra** имеет очень много механизмов защиты от неконсистентности данных.
## Hinted Handoff

Когда данные не могут быть доставлены на одну или несколько ответственных нод, нода координатор (которая получила данные), сохраняет их как `hinted handoff` (фактически файл) и как только нода координатор узнает о восстановлении потеряной ноды - она их ему передаст.
До тех пор пока вышедший из строя узел не получит `hinted handoff`, он не будет считаться восстановившемся. Это происходит автоматически в рамках [gracefull period](#^100002).